# REPRODUCIBILITY GUIDELINES

This repository accompanies a paper to facilitate its reproducibility: *"Evaluation of Models for Ranking of Long Documents"*.

It also has an online appendix, available by [this link](report/ECIR_2024__Understanding_performance_of_long_document_ranking_models_through_comprehensive_evaluation_and_leaderboarding__Appendix_.pdf)

**Important notes:** 

1. We provide scripts to train the models as well as all necessary configuration files.
2. To install the retrieval toolkit, please, use the [bootstrapping script](hist_scripts/bootstrap.sh). However, some collection-specific data needs to be created separately (see below).

The framework is used out-of-the-box and provides documentations regarding **installation**, indexing collections, and training 
the models.

Crucially:
1. The list of models with corresponding configuration files can be [found in this folder](model_conf).
2. The list of queries for both MS MARCO (documents) and Robust04 (with five folds) can be found [here](queries). This should simplify reproduction, because conversion of documents is relatively straightforward, but creation of training/testing folds for Robust04 is more involved.
3. The list of the so-called run files, which contain top-*K* candidates generated by the first-stage retriever can be [found in this folder](trec_runs_cached).
4. Upon acceptance we will make available already pre-processed (main) training set for MS MARCO (in the CEDR format). 

**Important note:** We cannot provided a prepared training set for Robust04, because it would contain documents, which cannot be accessed by the general public unless they [sign a licensing agreement with NIST](https://trec.nist.gov/data/cd45/index.html).

# Data processing and model training

The model training & evaluation pipeline has the following steps:
1. Downloading and indexing data (collection specific).
2. Organizing data into sub-folders and pointing environmental variable `COLLECT_ROOT` to the root-directory, 
containing (potentially multiple) collection sub-directories.
3. Creating indexes:
   1. retrieval index, e.g., Lucene
   2. forward index: here we always use the field `raw_text` whose field type is `textRaw'.
4. Exporting/preparing training data in the CEDR format (we provide exported data for MS MARCO).
5. Training the model.
6. Optionally validating it on a number of query sets, unless the exported data contains a complete testing
set itself.
7. In the case of Robust04 collections, we evaluate performance in the cross-validation mode. Thus,
one trains and validates a model for each fold and then merges output runs.

# MS MARCO
## Full data processing for MS MARCO v1 and v2

We experiment with two MS MARCO collections: v1 and v2. They have 
collection specific conversion scripts which can be found in respective sub-directories of the [data_convert](https://github.com/oaqa/FlexNeuART/tree/master/scripts/data_convert).
directory. A simplified training procedure (with precomputed training data) is described in the next sub-section.

Importantly, to generate training data and reproduce all results, one needs
to create a Lucene index that combines original lemmatized text with `doc2query` data. The respective scripts
can be found [here](https://github.com/oaqa/FlexNeuART/tree/master/scripts/data_convert/msmarco/add_doc2query_docs.py).
FlexNeuART input data is organized into multiple-fields, the default searchable field is `text`
and this is the field that needs to be expanded using `doc2query`. The expansion process 
creates a new text field, which can be then indexed using Lucene.

**Note on retrieval tookit field terminology:** although it may be worth renaming some fields in the future,
currently both the query and the document ID are encoded using the field name `DOCNO` (which sometimes causes confusion). 
The field names are the same, but the data files are different!
When `DOCNO` is present in `AnswerFields.jsonl` file, it is a document ID, when it is present 
in `QueryFields.jsonl` it denotes a query ID. 

For MS MARCO v1, candidate generator is BM25 on expanded representations.
For MS MARCO v2, we used a more involved `dense-sparse` retriever, which linearly combines BM25 with cosine similarity 
between [ANCE embeddings](https://github.com/microsoft/ANCE). 
We  provide the output of the first-stage retriever (in the form of a compressed run in TREC NIST format) for all test query sets,
but not for training sets.
Such runs are also provided for Robust04 v1 and [they are all stored in this folder of this repository](trec_runs_cached).
Note that FlexNeuART can use such runs directly as a replacement of the first-stage retriever (runs are retrieved using query IDs). 

## Robust04

Because Robust04 is not available for download (unless you sign an agreement), we cannot provide
a precomputed training set, but we provide the following:

1. Preprocessed [queries and qrels](queries/robust04).
2. [First-stage runs for fields `title` and `description`](trec_runs_cached/robust04).

The indexing/training pipeline for Robust04 is covered by generic FlexNeuART documentation.
One important difference though is that the data conversion uses [a generic conversion 
script](https://github.com/oaqa/FlexNeuART/blob/pypi2021/scripts/data_convert/ir_datasets/README.md),
which relies on [`ir_datases`](https://ir-datasets.com/), rather than a custom script as it is 
the case of MS MARCO.
